#### 什么是大型语言模型？

大型语言模型（LLM）是基于大量数据进行预训练的超大型深度学习模型。底层转换器是一组神经网络，这些神经网络由具有自注意力功能的编码器和解码器组成。编码器和解码器从一系列文本中提取含义，并理解其中的单词和短语之间的关系。

转换器 LLM 能够进行无监督的训练，但更精确的解释是转换器可以执行自主学习。通过此过程，转换器可学会理解基本的语法、语言和知识。

与早期按顺序处理输入的循环神经网络（RNN）不同，转换器并行处理整个序列。这可让数据科学家使用 GPU 训练基于转换器的 LLM，从而大幅度缩短训练时间。

借助转换器神经网络架构，您可使用非常大规模的模型，其中通常具有数千亿个参数。这种大规模模型可以摄取通常来自互联网的大量数据，但也可以从包含 500 多亿个网页的 Common Crawl 和拥有约 5700 万个页面的 Wikipedia 等来源摄取数据。

#### 大语言模型综述

语言是人类表达和交流的突出能力，它在儿童早期发展 并在一生中不断演变。然而，机器不能自然地掌握以人类语言形式理解和交流的能力，除非配备了强大的人工智能算法。实现这一目标，让机器像人类一样阅读、写作和交流一直是一个长期的研究挑战。

从技术上讲，语言建模是提高机器语言智能的主要方法之一。一般来说，语言建模旨在对词序列的生成概率进行建模，以预测未来(或缺失)单词的概率。语言建模的研究在文献中受到了广泛关注，可以分为四个主要发展阶段:

1. 统计语言模型 (SLM)

SLMs 基于统计学习方法 开发，并在 20 世纪 90 年代兴起。其基本思想是基于马尔可夫 假设建立词预测模型，例如根据最近的上下文预测下一个词。
具有固定上下文长度 n 的 SLM 也称为 n-gram 语言模型，例 如 bigram 和 trigram 语言模型。SLM 已被广泛应用于提高信 息检索和自然语言处理的任务性能。然而，它们通常受到维数灾难的困扰：由于需要估计指数级数量的转 换概率，因此很难准确估计高阶语言模型。因此，专门设计的平滑策略，如回退估计和 Good–Turing 估计已被引入以缓解数据稀疏问题。

2. 神经语言模型(NLM)

使用神经网络（例 如循环神经网络）来刻画词序列的概率。作为一个显著贡献，的工作引入了词的分布式表示概念，并在聚合上下文特征（即分布式词向量）的条件下构建词预测函数。通过扩展学习词或句子有效特征的想法，已有研究开发了一种通用神经

此外，word2vec被提出来构建一个简化的浅层神经网用于学习分布式词表示，这些表示在各种自然语言处理任务中被证明非常有效。这些研究开创了将语言模型用于表示学习（超越词序列建模），对自然语言处理领域产生了重要影响。

3. 预训练语言模型 (PLM)

作为早期尝试，ELMo被提出来通过预训练一个双向 LSTM（biLSTM）网络（而不是 学习固定的词表示）来捕捉上下文感知的词表示，然后根据特定的下游任务微调 biLSTM 网络。进一步，基于自注意力机制的高度并行化 Transformer 架构，BERT作为双向语言模型，在大规模无标签语料库上使用专门设计的预训练任务。

这些预训练的上下文感知词表示作为通用语义特征非常有效，极大地提高了自然语言处理任务的性能。这项研究激发了大量后续工作，确立了“预训练和微调”学习范式。
遵循这一范式，已经建立了大量关于预训练语言模型的研究引入了不同的架构（例如 GPT-2和 BART），或者改进的预训练策略。在这个范式中，通常需要对预训练语言模型进行微调以适应不同的下游任务。


4. 大语言模型 (LLM)

研究人员发现，扩展预训练语言模型（例如扩展模型大小或数据大小）通常会提高下游任务的模型容量（即遵循扩展定律）。许多研究通过训练越来越大的 PLM（例如 175B 参数的 GPT-3 和 540B 参数的 PaLM）来探索性能极限。尽管扩展主要在模型大小方面进行（具有类似的架构和预训练任务），但这些大尺寸的预训练语言模型表现出与较小的预训练语言模型（如 330M 参数的 BERT和 1.5B 参数的 GPT-2）不同的行为，
并在解决一系列复杂任务中展示了惊人的能力（称为涌现能力）。例如，GPT-3 可以通过上下文学习解决少样本任务，而 GPT-2 则表现不佳。因此，研究界为这些大型预训练语言模型命名为“大语言模型 (LLM)”1。LLM 的一个显著应用是 ChatGPT2，它将 GPT 系列的 LLM 应用于对话，展现了惊人的与人类对话的能力。 

在现有文献中，PLM 已经得到了广泛的讨论和调研，而很少有研究对 LLM 以系统的方式进行回顾。为了激发我们的调研，我们首先强调 LLM 和 PLM 之间的三个主要区别。首先，LLM 表现出一些令人惊讶的涌现能力，这些能力可能在以前较小的 PLM 中没有观察到。这些能力是语言模型 可能在以前较小的 PLM 中没有观察到。这些能力是语言模型 的强大和有效性。其次，LLM 将彻底改变人类开发和使用人 工智能算法的方式。与小型 PLM 不同，访问 LLM 的主要方法是通过提示接口(例如 GPT-4 API)。
人们必须了解 LLM的工作原理，并以 LLM 能够遵循的方式形式化他们的任务。第三，LLM 的发展不再明确区分研究和工程。训练 LLM 需要在大规模数据处理和分布式并行训练方面具有丰富的实践经验。为了开发出有能力的 LLM，研究人员必须解决复杂的工程问题，与工程师合作或成为工程师。 


如今，LLM 对 AI 社区产生了重大影响，ChatGPT 和GPT-4 的出现促使人们重新思考通用人工智能(AGI)的可能性。OpenAI 已经发布了一篇名为“Planning for AGI and beyond”的技术文章，讨论了实现 AGI 的短期和长期计划而一篇更近期的论文认为 GPT-4 可能被视为 AGI 系统的早期版本。AI 研究领域正因 LLM 的迅速发展而发生革命性变革。在自然语言处理领域，LLM 可以在一定程度上作为通用语言任务解决器，其研究范式已经转向使用 LLM。
在信 息检索领域，传统搜索引擎正受到通过 AI 聊天机器人(即ChatGPT)搜索新信息的挑战，而 New Bing3展示了一个初步的基于 LLM 增强搜索结果的研究尝试。在计算机视觉领域，研究人员试图开发类似 ChatGPT 的视觉-语言模型，以更好地为多模态对话提供服务GPT-4已经通过整合视觉信息支持多模态输入。这一新技术浪潮可能会带来 例如GPT-3、PaLM、Galactica和LLaMA。


具体而言，LLM 基于 Transformer 架构构建，其中多头注意力层堆叠在非常深的神经网络中。